{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMD/6HnjehavSzPuOzH9sfB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EkaterinaVoloshina/ASR_probing/blob/main/image_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W1PrcZNyePCh"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqeGhgzdRwJY",
        "outputId": "bf351ca7-b274-4aa5-fac2-ff67a15a0a4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Thesis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9YGewj9R5pV",
        "outputId": "78fe23e8-1406-4977-f499-b0264d0ca179"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Thesis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from tqdm.notebook import tqdm\n",
        "import csv\n",
        "import os\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import resnet50\n",
        "from itertools import combinations"
      ],
      "metadata": {
        "id": "xY4NPDRMWvw_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetLoader(Dataset):\n",
        "    def __init__(self, filename, dir_name=\"images\",\n",
        "                 image_size=256, device=\"cpu\", \n",
        "                 download=False, from_cache=False):\n",
        "        self.device = device\n",
        "        self.image_size = image_size\n",
        "        self.dir_name = dir_name\n",
        "        self.loader = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),  # scale imported image\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),])\n",
        "\n",
        "        if from_cache:\n",
        "            self.image_tensor, self.labels = self.load_from_cache(\n",
        "                dir_name=dir_name,\n",
        "                filename=filename\n",
        "            )\n",
        "        else:\n",
        "            annotations, images = self.open_file(filename)\n",
        "            self.image_tensor, self.labels = self.load_dataset(\n",
        "                images=images,\n",
        "                annotations=annotations,\n",
        "                download=download)\n",
        "            \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_tensor)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.image_tensor[idx], self.labels[idx]\n",
        "    \n",
        "    def load_from_cache(self, dir_name, filename):\n",
        "        annotations = pd.read_csv(filename)\n",
        "        labels = []\n",
        "        image_tensor = torch.Tensor(len(annotations), 3, self.image_size, self.image_size)\n",
        "        for num, image_name in enumerate(os.listdir(dir_name)):\n",
        "            img = Image.open(os.path.join(dir_name, image_name)).convert('RGB')\n",
        "            image = self.loader(img)\n",
        "            idx = image_name.split(\".\")[0]\n",
        "            image_tensor[num, :, :, :] = image\n",
        "            label = annotations[annotations[\"idx\"] == int(idx)].values.tolist()[0]\n",
        "            print(label)\n",
        "            labels.append(label)\n",
        "        return image_tensor, labels\n",
        "    \n",
        "    def open_file(self, filename):\n",
        "        with open(filename) as f:\n",
        "            content = json.load(f)\n",
        "        return (content[\"annotations\"],\n",
        "        content[\"images\"])\n",
        "\n",
        "    def open_and_save(self, url, filename=None, download=False):\n",
        "        response = requests.get(url)\n",
        "        try:\n",
        "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "            image = self.loader(img)\n",
        "            if download:\n",
        "                img.save(os.path.join(self.dir_name, f\"{filename}.jpg\"))\n",
        "            return image\n",
        "        except:\n",
        "            print(url)\n",
        "\n",
        "    def find_annotation(self, idx, annotations):\n",
        "        labels = []\n",
        "        for annotation in annotations:\n",
        "            if annotation[0][\"photo_flickr_id\"] == idx:\n",
        "                labels.append(annotation[0][\"original_text\"])\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def load_dataset(self, images, annotations, download=False):\n",
        "        labels = []\n",
        "        images = images[:50]\n",
        "        image_tensor = torch.Tensor(len(images), 3, self.image_size, self.image_size)\n",
        "        for num, image in tqdm(enumerate(images)):\n",
        "            idx = image[\"id\"]\n",
        "            label = self.find_annotation(idx, annotations)\n",
        "            if label != []:\n",
        "                img = self.open_and_save(image[\"url_o\"], \n",
        "                                         filename=image[\"id\"],\n",
        "                                         download=download)\n",
        "                if img != None: \n",
        "                    image_tensor[num, :, :, :] = img\n",
        "                    labels.append([str(idx), label[0], image[\"album_id\"]])\n",
        "        \n",
        "        image_tensor = image_tensor[:len(labels), :, :, :]\n",
        "\n",
        "        if download:\n",
        "            with open('labels.csv','w') as f:\n",
        "                w = csv.writer(f)\n",
        "                w.writerow((\"idx\", \"annotation\", \"album_id\"))\n",
        "                for label in labels:\n",
        "                    w.writerow(label)\n",
        "                \n",
        "        return image_tensor, labels    "
      ],
      "metadata": {
        "id": "bqPgQIv2Pcuk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderModel(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super(EncoderModel, self).__init__()\n",
        "        pretrained_model = resnet50(pretrained=True)\n",
        "        self.model = torch.nn.Sequential(*list(pretrained_model.children())[:-1])\n",
        "        self.adaptive_pool = torch.nn.AdaptiveAvgPool2d((img_size, img_size))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = x.max(-1).values.max(-1).values\n",
        "        return x"
      ],
      "metadata": {
        "id": "IQ7sZgiZPg-d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectors(data, model, device=\"cpu\"):\n",
        "    images = [] # change to tensor\n",
        "    annotations = [] \n",
        "    albums = []\n",
        "    ids = []\n",
        "    for image, (idx, annotation, album_id) in data:\n",
        "        image = image.to(device)\n",
        "        vector = model(image)\n",
        "        images.append(vector)\n",
        "        ids.append(idx)\n",
        "        annotations.append(annotation)\n",
        "        albums.append(album_id)\n",
        "    return images, ids, annotations, albums"
      ],
      "metadata": {
        "id": "A3a23gPWGlZh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confuse_captions(images, annotations, ids, album):\n",
        "\n",
        "    new_annotations = {}\n",
        "\n",
        "    cos_sim = cosine_similarity(images[0].detach().cpu().numpy())\n",
        "\n",
        "    —Åheck_album_id = lambda img1, img2: True if album[img1] == album[img2] else False\n",
        "\n",
        "    \n",
        "    for i, vector in enumerate(cos_sim):\n",
        "        closest = np.argsort(vector)\n",
        "        if closest[-1] == i:\n",
        "            closest_value_idx = -2\n",
        "        else:\n",
        "            closest_value_idx = -1\n",
        "\n",
        "        closest_value = closest[closest_value_idx]\n",
        "        while —Åheck_album_id(i, closest_value) and closest_value != -1:\n",
        "            closest_value_idx -= 1\n",
        "            closest_value = closest[closest_value_idx]\n",
        "        \n",
        "        new_annotations[ids[0][i]] = (annotations[0][i], annotations[0][closest_value])\n",
        "    \n",
        "    return new_annotations"
      ],
      "metadata": {
        "id": "MrB0z4yWbPNZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kakaUXvM-PIS",
        "outputId": "8bfe71d1-3fb2-4f34-ecff-732cc71060e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äòimages‚Äô: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loader = DatasetLoader(filename=\"dii/test.description-in-isolation.json\")\n",
        "#                       download=True)\n",
        "loader = DatasetLoader(filename=\"labels.csv\", dir_name=\"images\", from_cache=True)\n",
        "data = torch.utils.data.DataLoader(loader, batch_size=32)\n",
        "model = EncoderModel(256).to(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3lj_t6drqPN",
        "outputId": "52a34bd4-f1f2-4172-832a-181b8c68bc44"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1741642, 'The sign is describing when the services will begin.', 44277]\n",
            "[1741587, 'A man in a top hat has a magic trick on the floor.', 44277]\n",
            "[1741622, 'A older man with a black hat, mustache and glasses.', 44277]\n",
            "[1741640, 'Sitting there waiting on someone to come over and buy something.', 44277]\n",
            "[1741632, 'a case full of books in a house, books appear to be old', 44277]\n",
            "[355205, 'Taken at some sort of carnival, the camera captured the movement and lights of the amusement ride.', 8139]\n",
            "[355331, 'Large stuffed neon ape toys hang from the ceiling of a carnival game.', 8139]\n",
            "[355208, 'two children riding on a dragon roller coaster', 8139]\n",
            "[355204, 'Two girls smiling while sitting in a cart for a carnival ride.', 8139]\n",
            "[355332, 'A mother and her daughters look at a carnival game.', 8139]\n",
            "[21728852, 'Furry animals being pet by some people inside a building', 504823]\n",
            "[21725505, 'A small car is demoed at a show under blue lighting.', 504823]\n",
            "[21731442, 'the guy in pink shirt is riding a mowped', 504823]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images, ids, annotations, albums = get_vectors(data, model)"
      ],
      "metadata": {
        "id": "J2Pw3-PubBnu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]"
      ],
      "metadata": {
        "id": "JRB-CR4M21A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_ann = confuse_captions(images, annotations, ids, albums[0])\n",
        "\n",
        "with open('annotations.csv','w') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerows(table_ann.items())"
      ],
      "metadata": {
        "id": "VB1pOUAxzl5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}